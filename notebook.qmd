---
title: Claude Export Analysis
format:
    html:
        code-fold: true
jupyter: python3
---

This is a notebook for processing the Claude data export to determine which of your conversations with the chatbot are valuable and worth archiving for future reference.

It's inspired by the [claude-export](https://github.com/eudoxia0/claude-export/) notebook by Github user [eudoxia0](https://github.com/eudoxia0).

It uses the [DeepSeek V3](https://platform.deepseek.com/) API to analyze the conversations. DeepSeek's API is extremely low-cost and high-capacity, so we can parallelize the analysis and quickly get a result for hundreds of conversations at a cost of just a few cents per hundred conversations.

# Parsing

```{python}
import json
from dataclasses import dataclass
from datetime import datetime, UTC
import re
import os
from dotenv import load_dotenv
from litellm import completion
import time
import pandas as pd
from pydantic import BaseModel
from typing import List
from concurrent.futures import ThreadPoolExecutor
from functools import partial

@dataclass(frozen=True)
class Attachment:
    file_name: str
    file_size: int
    file_type: str
    extracted_content: str

    @staticmethod
    def from_json(data):
        return Attachment(
            file_name=data["file_name"],
            file_size=data["file_size"],
            file_type=data["file_type"],
            extracted_content=data["extracted_content"],
        )

@dataclass(frozen=True)
class Files:
    file_name: str

    @staticmethod
    def from_json(data):
        return Files(
            file_name=data["file_name"],
        )

@dataclass(frozen=True)
class Message:
    uuid: str
    text: str
    sender: str
    created_at: str
    attachments: list[Attachment]
    files: list[Files]

    @staticmethod
    def from_json(data):
        return Message(
            uuid=data["uuid"],
            text=data["text"],
            sender=data["sender"],
            created_at=data["created_at"],
            attachments=[Attachment.from_json(d) for d in data["attachments"]],
            files=[Files.from_json(d) for d in data["files"]],
        )

    def is_human(self):
        return self.sender == "human"

    def wordcount(self):
        return len(re.findall(r'\w+', self.text))

    def bytecount(self):
        return len(self.text)

@dataclass(frozen=True)
class Conversation:
    uuid: str
    name: str
    created_at: datetime
    updated_at: datetime
    messages: list[Message]

    @staticmethod
    def from_json(data):
        return Conversation(
            uuid=data["uuid"],
            name=data["name"],
            created_at=datetime.fromisoformat(data["created_at"]),
            updated_at=datetime.fromisoformat(data["updated_at"]),
            messages=[Message.from_json(d) for d in data["chat_messages"]],
        )

def load_data() -> list[Conversation]:
    with open("inputs/conversations.json", "r") as stream:
        data = json.load(stream)
        cs = [Conversation.from_json(d) for d in data]
        cs = sorted(cs, key=lambda c: c.created_at)
        cs = [c for c in cs if c.messages]
        return cs

cs: list[Conversation] = load_data()
```

# Conversation Value Analysis

```{python}
# Load environment variables
load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

# Add rate limiting constants
RATE_LIMIT_DELAY = 0.5  # seconds between API calls

class ConversationAnalysis(BaseModel):
    """Model for conversation value analysis response"""
    score: int
    reasoning: str

def get_value_score(self) -> tuple[int, str]:
    """
    Analyzes the entire conversation to determine its future reference value for generating educational content or interesting research projects.
    Returns tuple of (score, explanation)
    """
    response = completion(
        model="deepseek/deepseek-chat",
        messages=[
        {
            "role": "system",
            "content": "You are an AI trained to evaluate conversation content for its future reference value for generating educational content or interesting research projects."
        }, {
            "role": "user",
            "content": f"""Analyze this conversation and rate its value for future reference:

            Conversation:
            {self.messages}

            Consider:
            - Technical depth and complexity
            - Reusability of solutions
            - Novel insights or approaches
            - Educational value
            
            Output your response in JSON format, with a reasoning field in which you think through whether the conversation is worth archiving for future reference and why, and a score field with your integer score. The score should be between 0 and 10, where 0 is the lowest value and 10 is the highest value.

            Example JSON output for a conversation that is not at all valuable:
            {{
                "reasoning": "The conversation is a grinding, repetitive debugging session specific to one rather idiosyncratic project and does not provide any useful learning or even a resolution of the problem.",
                "score": 0
            }}

            Example JSON output for a conversation that is somewhat valuable:
            {{
                "reasoning": "While not revolutionary, the conversation analyzes a social media trend and contains one or two insightful observations about how ideas spread. Could maybe be used as a case study or example in a future research project.",
                "score": 5
            }}

            Example JSON output for a conversation that is highly valuable:
            {{
                "reasoning": "The conversation breaks new ground in the field of AI and provides a number of extraordinary insights that could drive an impactful future research project.",
                "score": 10
            }}"""
        }],
        format="json",
        api_key=DEEPSEEK_API_KEY
    )
    # Extract just the JSON content from the response
    content_dict = response.choices[0].message.content
    
    # Remove any markdown formatting if present
    if content_dict.startswith('```'):
        # Find the first and last occurrence of '```' and extract content between them
        start = content_dict.find('\n') + 1
        end = content_dict.rfind('```')
        content_dict = content_dict[start:end].strip()
    
    # Then validate the properties object against our model
    result = ConversationAnalysis.model_validate(json.loads(content_dict))
    time.sleep(RATE_LIMIT_DELAY)
    return (result.score, result.reasoning)

Conversation.get_value_score = get_value_score

def analyze_single_conversation(conversation, index, total):
    """Analyze a single conversation with progress tracking"""
    print(f"Analyzing conversation {index}/{total}: {conversation.name}")
    try:
        score, reasoning = conversation.get_value_score()
        return (conversation.name, score, reasoning, conversation.created_at)
    except Exception as e:
        print(f"Error analyzing conversation {index}: {e}")
        return (conversation.name, 0, f"Error: {str(e)}", conversation.created_at)

def analyze_conversation_value():
    """Analyze and rank conversations by their potential future value using parallel processing"""
    print("Analyzing conversations in parallel... This may take a while.")
    
    total = len(cs)
    
    # Use ThreadPoolExecutor for parallel processing
    max_workers = min(10, total)  # Adjust max_workers based on your API limits
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Create iterator of conversations with their indices
        conversations_with_index = zip(cs, range(1, total + 1))
        # Map the worker function directly without using partial
        scores = list(executor.map(
            lambda x: analyze_single_conversation(x[0], x[1], total), 
            conversations_with_index
        ))

    # Create DataFrame
    df = pd.DataFrame(scores, columns=['Title', 'Value Score', 'Reasoning', 'Created At'])

    # Sort by value score descending
    df = df.sort_values('Value Score', ascending=False)

    # Reset index for ranking
    df = df.reset_index(drop=True)
    df.index = df.index + 1

    return df

if os.path.exists("outputs/conversation_value_analysis.csv"):
    # Load the conversation ratings from CSV file if it exists
    df = pd.read_csv("outputs/conversation_value_analysis.csv")
else:
    # Run the analysis and save the results to CSV file
    df = analyze_conversation_value()
    df.to_csv("outputs/conversation_value_analysis.csv", index=False)

df['Created At'] = pd.to_datetime(df['Created At'])
assert not df['Created At'].isna().any(), "Created At column contains NaT values"

# Sort by date
df = df.sort_values('Created At')

df.head(10)
```

``` {python}
import matplotlib.pyplot as plt
import seaborn as sns

# Create a column for the moving average of the value score
df['Moving Average'] = df['Value Score'].rolling(window=40).mean()

# Plot moving average of conversation value over time
plt.figure(figsize=(10, 6))
sns.lineplot(x='Created At', y='Moving Average', data=df)
plt.title('Conversation Value Over Time (30-day Moving Average)')
plt.xlabel('Date')
plt.ylabel('Average Value Score')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("outputs/conversation_value_over_time.png")
```
